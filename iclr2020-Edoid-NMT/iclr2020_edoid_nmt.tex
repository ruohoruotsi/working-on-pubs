
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}

\title{Towards Neural Machine Translation \\ for Edoid Languages}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Iroro Fred \d{\`O}n\d{\`o}m\d{\`e} Orife \\
Niger-Volta Language Technologies Institute\\
Seattle, Washington, USA \\
\texttt{iroro@alumni.cmu.edu} \\
\And
Dr. John N. Orife\\
Indiana University of Pennsylvania, \\
Indiana, Pennsylvania, USA \\
\texttt{jorife@iup.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Many Nigerian languages have relinquished their previous prestige and purpose in modern society to English and Nigerian Pidgin. For the millions of L1 speakers of indigenous languages, there are inequalities that manifest themselves as unequal access to information, communications, health care, security as well as attenuated participation in political and civic life. To minimize exclusion and provide more access, this work explores the feasibility of Neural Machine Translation (NMT) for the Edoid language family of Southern Nigeria. Using the new JW300 public dataset, we trained and evaluated baseline translation models for four widely spoken languages in this group: \d{\`E}d{\'o}, \d{\`E}s{\'a}n, Urhobo and Isoko. Trained models, code and datasets have been open-sourced to advance future research efforts on Edoid language technology.\\
\end{abstract}

\section{Introduction}

% How language inequality came about
Nigerian languages with primarily oral traditions have been the most susceptible to language endangerment. Numerous have already ceded privileged positions in society to English or Nigerian Pidgin. Unlike many East and South Asian societies, which preserved the socio-linguistic status of their indigenous languages for centuries under colonial rule, Nigerian tongues have not fared as well.
% How language inequality affects people and indigenous language research
For L1 speakers, this language inequality is further exacerbated in a technological age, where only the most highly resourced (i.e. colonial) languages become the milieu for access to information, telecommunications, health care, security, economic advancement as well as participation in political and civic life \cite{odojelanguage, awobuluyi201626}. There are practical and technical challenges with respect to orthographic standardizations, consistent diacritic representation (Unicode) in electronic media and across device types. 

% How can NMT help?
For almost-extinct languages, machine translation offers technology for language documentation and preservation. For speakers of minority Nigerian languages, it can facilitate good governance, national development and offers a path for technological, economic, social and political participation and empowerment to those with unequal access \citep{odoje201612, odojelanguage}. % based on economic status, demography, location and language

The objective of this paper is to report foundational NMT work to assist translators and the lay-person alike working in Edoid languages. We hope to bootstrap the development and sustenance of scholarly and literary traditions, beyond religious texts. By making our models and code broadly accessible as open-source projects, we hope to energize academic and industry interest while contributing tools of self- solution of disenfranchisement based on language and socio-economic barriers.



%From a research vantage, these languages have been labelled ``low resource" due to the scarcity of academic research, online-datasets, funding and interest, due to perceptions about the prestige and utility of these languages in contemporary African life  
% Among other factors, the scarcity of highly esteemed works of literary scholarship has worked to devalue indigenous languages, making them susceptible to being replaced by other languages of power and status \citep{awobuluyi201626}. 

\subsection{Languages}
Belonging to the Volta-Niger family, and spoken by some 5 million people, the Edoid languages of Southern Nigeria (Edo and Delta State) comprise over two dozen so-called ``minority" languages. The term \emph{Edoid} stems from \d{\`E}d{\'o}, the most broadly spoken member langauge and the language of the famed Kingdom of Benin. Urhobo and Isoko are classified as South-Western Edoid, while \d{\`E}d{\'o}, \d{\`E}s{\'a}n are classified as North-Central \citep{wikiedoid}.

% Stats on population and literacy rates? other linguistic  
% what do they have in common, what is different amonst them. Summarize
All these languages and tonal in nature, ie. have certain grammatical and tonal relationships, blah blah blah

We contrast the performance of a baseline Transformer model across the four languages under study, examining the effect of word-level versus subword-level tokenization.


% Strong oral traditions, mostly written works are Bibles, hymm books and dictionaries.
%https://en.wikipedia.org/wiki/Urhobo-Isoko

\subsection{Related Works}

While there has been recent interest in NMT for African languages, in Nigeria there has been a bit of literature on Rule-based, phrase-based and Statistical machine translation. This is the first work known to the authors done in any of the Edoid langauges specifically for machine translation.


\section{Methodology}
\label{methods}

\subsection{Dataset}
The recently released JW300 dataset is a large-scale, parallel corpus for Machine Translation (MT) comprising more than three hundred languages of which 101 are African \citep{agic-vulic-2019-jw300}. English-\{\d{\`E}d{\'o}, \d{\`E}s{\'a}n, Urhobo, Isoko\} token pairs cardinality is itemized in Table~\ref{results}.  \{10200, 2000, 200, 4000\} respectively. JW300 text is drawn from a number of online blogs, news and contemporary religious magazines by Jehovah's Witnesses (JW).
\subsection{Models}

We used the JoeyNMT framework to train the Transformer. We built all models with the Python 3 implementation of \texttt{JoeyNMT}, an open-source toolkit created by the Klein et al. \citep{opennmt}. Our training hardware configuration was a standard AWS EC2 p2.xlarge instance with a NVIDIA K80 GPU, 4 vCPUs and 61GB RAM. Training the various models took place over the course of a few days.

\section{Results}
\label{results}



\begin{table}[h]
\caption{Per-language BLEU scores by BPE or word-level tokenization}
\label{results}
\begin{center}
\begin{tabular}{c@{\qquad}ccc@{\qquad}ccc}
  \toprule
  \multirow{2}{*}{\raisebox{-\heavyrulewidth}{\textbf{Language}}} & \multicolumn{2}{c}{\textbf{BPE}} & \multicolumn{2}{c}{\textbf{Word}} & \multirow{2}{*}{\raisebox{-\heavyrulewidth}{\textbf{Training Tokens}}} \\
  \cmidrule{2-5}
  & dev & test & dev & test \\
  \midrule
  \d{\`E}d{\'o}  & 7.92 & 12.49 & 5.99 & 8.24 &  229,307\\
  \d{\`E}s{\'a}n & 4.94 & 6.25 & 3.39 & 5.30 & 87,025\\
    \midrule
  Urhobo  & 15.91 & 28.82 & 11.80 & 22.39 & 519,981 \\
  Isoko   & 32.58 & 38.05 & 32.38 & 38.91 & 4,824,998 \\
  \bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Qualitative}

% Dad's view on the concepts 
Unsuprisingly, for Urhobo and Isoko which are much better resourced, the BLEU scores are generally correlated with the translation quality when reviewed by L1 speakers. For example, for Urhobo this translation captures much o

Second level headings are in small caps,
flush left and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsection{Error Analysis}
While performing error analyses on the model predictions, we observed: DESCRIBE YOUR OBSERVATIONS

\section{Conclusions}


\subsection{Discussion}
Additional data and more diverse data definitely improves performance. Modern Text embeddings will also provide an additional boost in accuracy. Overall more studies are needed regarding algorithmic preprocessing and hyperparamter fine-tuning. For example, we naively saw that for the smaller corpora BPE tokenization gave a slight boost in BLEU performance, while 


\subsection{Future Work}
We see this work as a foundational effort with numerous avenues for future exploration, with the objective of moving quickly from pure research to applications. These include 

Backtranslation, hyper-parameter optimization, character, sub-word and word-level tokenization experiments. 
 
 Thee include social justice by addressing an aspect of technological language inequality, language perservation and by establishing baselines and from which to build on. Given the comparatively low (Oladele Awobuluyi) litearay traditions but the very strong oral traditions, foundational language technologies based on good clean text, like language and translation models are just the start, but very important precusor to speech interfaces. Imagine a world in which a culture rooted in a strong oral tradition can make use of Speech-to-Speech interfaces, speaking and being spoken to idiomatically. This is where the future of African langauge technology lies and mahcine translation and good clean datasets are the core.   

% Applications to film and cinema technology to automatically subtitle and translate Edoid language films.

% paint a future where robust speech-to-speech interfaces kick ass and allow everyone to speak freely


All public-domain datasets referenced in this work are available on GitHub.\footnote{\url{https://github.com/Niger-Volta-LTI}}

\subsubsection*{Acknowledgments}
The authors thank Dr. Ajovi B. Scott-Emuakpor, MD and Dr. John Nevboyeri Orife for their encouragement, support and qualitative translations.

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}
\appendix
\section{Appendix}
You may include example translations. 

\end{document}
