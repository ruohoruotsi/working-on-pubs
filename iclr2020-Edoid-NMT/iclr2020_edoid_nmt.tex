
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}

\title{Towards Neural Machine Translation \\ for Edoid Languages}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Iroro Fred \d{\`O}n\d{\`o}m\d{\`e} Orife \\
Niger-Volta Language Technologies Institute\\
Seattle, Washington, USA \\
\texttt{iroro@alumni.cmu.edu} \\
\And
Dr. John N. Orife\\
Indiana University of Pennsylvania, \\
Indiana, Pennsylvania, USA \\
\texttt{jorife@iup.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Many Nigerian languages have lost their previous prestige and purpose in modern society due the status of English and Nigerian Pidgin. These language inequalities for L1 speakers manifest themselves in unequal access to information, connectivity, health care, security as well as attenuated participation in political and civic life. This work explores the feasibility of Neural Machine Translation (NMT) for the Edoid languages family of Southern Nigeria. Using public datasets, we trained and evaluated translation models for four widely spoken langauges in this group: \d{\`E}d{\'o}, \d{\`E}s{\'a}n, Urhobo and Isoko. Trained models, code and datasets have been open-sourced to advance future research efforts on language technology for this minority language family.\\


\end{abstract}

\section{Introduction}
Language technology has an enabling effect on society. However computational linguistics research has only addressed about 1\% of the world's languages. \citep{bird2012machine}. The disparity in 

\citep{odoje201612}

Good Goveranance, language equality, access to information and the such.

Machine translation  is relevant to the process of language documentation, preservation

\subsection{Languages}
Belonging to the Volta-Niger family, and spoken by some 5 million people, the Edoid languages of Southern Nigeria (Edo and Delta State) comprise over two dozen so-called ``minority" languages. The term \emph{Edoid} comes from \d{\`E}d{\'o}, the language of the famed Kingdom of Benin and the most broadly spoken member. The two langauges under study, Urhobo and Isoko, are classified as South-Western Edoid, while \d{\`E}d{\'o}, \d{\`E}s{\'a}n are classified as North-Central \citep{wikiedoid}.

% Stats on population and literacy rates? other linguistic  
% what do they have in common, what is different amonst them. Summarize
Most African languages have been labelled ``low resource". This is due in part to the limited body of academic research, lack of funding and online-datasets as well as low interest, due to preceptions about the prestige and utility of these languages in contemporary African life \cite{odojelanguage, awobuluyi201626}. Finally there are practical and technical challenges with respect to orthographic standardizations, consistent diacritic representation (Unicode) in electronic media and across device types. 

work has been done on Edoid languages, this is in part due to the limited datasets, which is a function of a 

The objective of this study is to carry out foundational work using current NMT techniques in language family with rich oral traditions but very little literature. We seek to energize interest in language technology research for these minority languages. We contrast the performance of a baseline Transformer model across the four languages under study, examining the effect of word-level versus subword-level tokenization.



% Why is this an asset to the whole MT community?
% NMT performance as measured in WMT benchmark has plateaued and there is little to no difference between competing systems when evaluated on resource-rich languages. Massively-multilingual approaches are up and coming, but they require the existence of related large-scale resources for low-resource languages to benefit. (back this up by a citation!) 
% Pseudo-low-resource data sets are used to evaluate ``low-resource'' approaches, because the access to truly low-resource data requires much more than just downloading a data set. The assets that Masakhane is creating, be it the experience in building a community, distributed research, or expertise in NLP for African languages specifically, will help MT researchers to work on new frontiers and aim for meaningful progress. (not just BLEU, uargh)

%  All pre-trained models, datasets and source-code have been released as an open-source 

% Strong oral traditions, mostly written works are Bibles, hymm books and dictionaries.
%https://en.wikipedia.org/wiki/Urhobo-Isoko

\subsection{Related Works}

While there has been recent interest in NMT for African languages, in Nigeria there has been a bit of literature on Rule-based, phrase-based and Statistical machine translation. This is the first work known to the authors done in any of the Edoid langauges specifically for machine translation.


\section{Methodology}
\label{methods}

\subsection{Dataset}
The recently released JW300 dataset is a large-scale, parallel corpus for Machine Translation (MT) comprising more than three hundred languages of which 101 are African \citep{agic-vulic-2019-jw300}. English-\{\d{\`E}d{\'o}, \d{\`E}s{\'a}n, Urhobo, Isoko\} token pairs number \{10200, 2000, 200, 4000\} respectively. JW300 text is drawn from a number of online blogs, news and contemporary religious magazines by Jehovah's Witnesses (JW).
\subsection{Models}

We used the JoeyNMT framework to train the Transformer. We built all models with the Python 3 implementation of \texttt{JoeyNMT}, an open-source toolkit created by the Klein et al. \citep{opennmt}. Our training hardware configuration was a standard AWS EC2 p2.xlarge instance with a NVIDIA K80 GPU, 4 vCPUs and 61GB RAM. Training the various models took place over the course of a few days.

\section{Results}
\label{results}



\begin{table}[h]
\caption{Evaluation BLEU scores}
\label{tab:results}
\centering
\begin{tabular}{c@{\qquad}ccc@{\qquad}ccc}
  \toprule
  \multirow{2}{*}{\raisebox{-\heavyrulewidth}{\textbf{Language}}} & \multicolumn{2}{c}{\textbf{Word-level}} & \multicolumn{2}{c}{\textbf{BPE}} & \multirow{2}{*}{\raisebox{-\heavyrulewidth}{Training Tokens}} \\
  \cmidrule{2-5}
  & dev & test & dev & test \\
  \midrule
  \d{\`E}d{\'o}  & 10.0  & 0.01 & 30.1 & 11.8 &  100,000\\
  \d{\`E}s{\'a}n & 11.22 & 20.4 & 30.9 & 11.9 & 300,300\\
    \midrule
  Urhobo  & 11.33 & 1.342 & 33.4 & 11.2 & 3,000,000 \\
  Isoko   & 11.22 & 12.99 & 13.4 & 11.7 & 4,000,000 \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{Qualitative}

% Dad's view on the concepts 
Unsuprisingly, for Urhobo and Isoko which are much better resourced, the BLEU scores are generally correlated with the translation quality when reviewed by L1 speakers. For example, for Urhobo this translation captures much o

Second level headings are in small caps,
flush left and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsection{Error Analysis}
While performing error analyses on the model predictions, we observed: DESCRIBE YOUR OBSERVATIONS

\section{Conclusions}


\subsection{Discussion}
Additional data and more diverse data definitely improves performance. Modern Text embeddings will also provide an additional boost in accuracy. Overall more studies are needed regarding algorithmic preprocessing and hyperparamter fine-tuning. For example, we naively saw that for the smaller corpora BPE tokenization gave a slight boost in BLEU performance, while 


\subsection{Future Work}
We see this work as a foundational effort on a few fronts. Thee include social justice by addressing an aspect of technological language inequality, language perservation and by establishing baselines and from which to build on. Given the comparatively low (Oladele Awobuluyi) litearay traditions but the very strong oral traditions, foundational language technologies based on good clean text, like language and translation models are just the start, but very important precusor to speech interfaces. Imagine a world in which a culture rooted in a strong oral tradition can make use of Speech-to-Speech interfaces, speaking and being spoken to idiomatically. This is where the future of African langauge technology lies and mahcine translation and good clean datasets are the core.   

All public-domain datasets referenced in this work are available on GitHub.\footnote{\url{https://github.com/Niger-Volta-LTI}}

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\end{document}
