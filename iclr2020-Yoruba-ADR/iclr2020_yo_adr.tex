
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}

\title{Improving Yor{\`u}b{\'a} Diacritic Restoration}


% This will be everyone who has (a) provided or cleaned text (b) contributed code
%% TODO Everyone put their Affiliation, Take away Niger-Volta LTI, if you want, it was just there as a placeholder, given I don't know everyone's desired affiliation

\author{David Ad{\'e}lan{\'i} \\
Saarland University\\
Niger-Volta LTI \\
\And
Iroro Fred \d{\`O}n\d{\`o}m\d{\`e} Orife \\
Niger-Volta LTI\\
\And
K\d{\'{o}}l\'{a} T\'{u}b\d{\`{o}}s\'{u}n \\
Yor{\`u}b{\'a} Name \\
\And
T{\`i}m{\'i}l\d{\'{e}}h{\`i}n Fasubaa \\
Niger-Volta LTI \\
\And
\d{O}l{\'a}mil{\'e}kan Wahab \\
Niger-Volta LTI \\
\And
W{\'u}r{\`a}\d{o}l{\'a} Oy{\`e}w{\`u}s{\`i} \\
Data Science Nigeria \\
Niger-Volta LTI \\
\And
Victor Williamson \\
Yor{\`u}b{\'a} Name \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

% \begin{abstract}
% Yor{\`u}b{\'a} is a widely spoken West African language with a writing system rich in orthographic and tonal diacritics. They provide morphological information, are crucial for lexical disambiguation, pronunciation and are vital for any computational Speech or Natural Language Processing  tasks. However diacritic marks are commonly excluded from electronic texts due to limited device and application support as well as general education on proper usage. Building on our previous work, we report on recent efforts at dataset cultivation. By aggregating and improving disparate texts from the web and various personal libraries, we were able to significantly grow our clean Yor{\`u}b{\'a} dataset from a majority Bibilical text corpora from three sources to millions of tokens from over a dozen sources. We evaluate updated diacritic restoration models on a new, general purpose, public domain Yor{\`u}b{\'a} dataset of modern journalistic news text, selected to be a multi-purpose corpus reflecting contemporary usage. All pre-trained models, datasets and source-code have been released as an open-source project to advance efforts on Yor{\`u}b{\'a} language technology.\\
% \end{abstract}

\section{Introduction}\label{sec:introduction}

Yor{\`u}b{\'a} is a tonal language spoken by more than 40 Million people in the countries of Nigeria, Benin and Togo in West Africa. The phonology is comprised of eighteen consonants, seven oral vowel and five nasal vowel phonemes with three kinds of tones realized on all vowels and syllabic nasal consonants \citep{akinlabi2004sound}. Yor{\`u}b{\'a} orthography makes notable use of tonal diacritics, known as \emph{am{\'i} oh{\`u}n}, to designate tonal patterns, and orthographic diacritics like underdots for various language sounds \citep{adegbola2012quantifying, wells2000orthographic}.

Diacritics provide morphological information, are crucial for lexical disambiguation, pronunciation and are vital for any computational Speech or Natural Language Processing  task. To unlock the potential for a robust ecosystem of \emph{Yor{\`u}b{\'a}-first} language technologies, Yor{\`u}b{\'a} text must be correctly represented on current and future computing environments. The ultimate objective of automatic diacritic restoration (ADR) systems is to facilitate text entry and text correction that motivates and encourages the correct orthography and promotes quotidian usage of the language in electronic media.

\subsection{Ambiguity in non-diacritized text}
The main challenge in non-diacritized text is that it is very ambiguous \citep{orife2018adr, adegbola2012quantifying, asahiah2017restoring, de2007automatic}. ADR attempts to decode the ambiguity present in undiacritized text. Adegbola et al. assert that for ADR the ``prevailing error factor is the number of valid alternative arrangements of the diacritical marks that can be applied to the vowels and syllabic nasals within the words" \citep{adegbola2012quantifying}. 
% Analogously to the analyses performed in \citep{orife2018adr}, we quantify the ambiguity in our current training set by the percentage of all words that have diacritics, 85\%; the percentage of unique non-diacritized word types that have two or more diacritized forms, 32\%, and the lexical diffusion or \emph{LexDif} metric, which conveys the average number of alternatives for each non-diacritized word, 1.47.
% TODO IRORO, to provide the above figures. Not sure if we need to be soo wordy given the template is 2 pages + references + Appendix.

\begin{table}[h]
\caption{Diacritic characters with their non-diacritic forms}
\label{ambiguity-table}
\begin{center}
  \begin{tabular}{lcl}
    \multicolumn{2}{c}{\bf Characters} & \textbf{Examples}  \\
	 \hline \\
    {\`a} {\'a} \v{a} & \textbf{a} & gb{\`a} \emph{(spread)}, gba \emph{(accept)}, gb{\'a} \emph{(hit)}    \\  
    {\`e} {\'e} \d{e} \d{\`e} \d{\'e} & \textbf{e} & {\`e}s{\`e} \emph{(dye)}, \d{e}s\d{\`e} \emph{(foot)}, es{\'e} \emph{(cat)}\\
    {\`i} {\'i} & \textbf{i} & {\`i}l{\`u} \emph{(drum)}, ilu \emph{(opener)}, {\`i}l{\'u} \emph{(town)}\\  
    {\`o} {\'o} \d{o} \d{\`o} \d{\'o} \v{o} & \textbf{o} & ar\d{o} \emph{(an invalid)}, ar{\'o} \emph{(indigo)}, {\`a}r{\`o} \emph{(hearth)}, {\`a}r\d{o} \emph{(funnel)}, {\`a}r\d{\`o} \emph{(catfish)}\\  
    {\`u} {\'u} \v{u} & \textbf{u} & k{\`u}n \emph{(to paint)}, kun \emph{(to carve)}, k{\'u}n \emph{(be full)} \\
	\hline \\
    {\`n} {\'n} \={n} & \textbf{n} & {\`n} (a negator), {n} \emph{(I)}, {\'n} (continuous aspect marker) \\  
    \d{s} & \textbf{s} &  \d{s}{\`a} \emph{(to choose)}, \d{s}{\'a} \emph{(fade)}, {s}{\`a} \emph{(to baptise)}, {s}{\'a} \emph{(to run)}\\  
	\hline \\
  \end{tabular}
\end{center}
\end{table}

% Finally, 64\% of all unique, non-diacritized monosyllabic words possess multiple diacritized forms \citep{oluseye2003yoruba, delano1969dictionary}. Given that Yor{\`u}b{\'a} verbs are predominantly monosyllabic and that there are tonal changes rules governing how tonal diacritics on a specific word are \emph{altered} based on context, we acknowledge the complexity of the disambiguation task of diacritic restoration \citep{orife2018adr, delano1969dictionary}. 

\subsection{Improving generalization performance}

In our efforts to make the the first ADR models \citep{orife2018adr}, trained on majority Biblical text, available to a wider audience, we frequently tested on colloquial, conversational text. We observed that these early ADR models suffered from domain-mismatch generalization errors and appeared particularly not-robust when presented with contractions or variants of common phrases. We attributed these errors to low-diversity of sources and an insufficient number of training examples. To remedy this problem,  we set about to  we demonstrate that we can improve the model trained on majority Biblical text, using a lot more text from the a variety of sources.

\section{Methodology}\label{sec:methods}

\paragraph{Data Collection}\label{sec:collection}

We started by first attempting a comprehensive GitHub, literature and Google web search, assembling all the public-domain Yor{\`u}b{\'a} sources online. Those without admissible or consistent quality were put into a special queue for human supervision and corrections. This notably included Wikipedia and Twitter. TIMI OCR SENTENCE HERE. In Table ~\ref{tab:training_datasets} we summarize each of the admitted corpora to convey a sense of the subject matter and word distributions. 


% \begin{itemize}
% \item \textbf{JW300}: JW300 is a large-scale a parallel corpus for Machine Translation (MT),  by Jehovah's Witnesses (JW), comprising more than three hundred languages with on average one hundred thousand parallel sentences per language pair. The Yor{\`u}b{\'a}-English token pairs number some thirteen million. JW300 text is drawn from a number of online blogs, news and contemporary religious magazines \citep{agic-vulic-2019-jw300}.

% \item \textbf{B{\'i}b{\'e}l{\`i}}: Two different versions of the Yor{\`u}b{\'a} Bible were web-scraped cleaned. The first is from Biblica, which is a translation of the New International Version (NIV). The second version is a 2010 translation published by the Bible Society of Nigeria (BSN). Our previous work only used the Biblica version. A computational analysis of the word distributions of each Bible's text revealed that on average 60\% of all verses were not identical. Therefore, it valuable to have both versions of the Bible to give a diversity of expression for the same concepts. Further more, the additional size was is still dwarfed by the JW300 corpus. 

% \item \textbf{Language identification corpus}: This medium sized corpus of one hundred and fifty thousand tokens was used in a Nigerian language identification task involving the three major Nigerian tongues, Hausa, Igbo and Yor{\`u}b{\'a}. We obtained the public-domain text from GitHub \citep{Asubiaro_langid}.

% \item \textbf{Yor{\`u}b{\'a}-Twi word-embedding corpora}
% This medium sized corpus comprises a number of texts used in a recent word embedding task 

% \item \textbf{{\`O}we} is a small collection of Yor{\`u}b{\'a} proverbs scraped from the electronic version of \emph{The Good Person: Excerpts from the Yor{\`u}b{\'a} Proverb Treasury}, a collection compiled and translated by Dr. Oyekan Owomoyela at the University of Nebraska - Lincoln. The corpus has five thousand tokens and parallel English translations \citep{oweyoruba}.

% \item \textbf{Universal Declaration of Human Rights} is a tiny corpus of some two thousand eight hundred tokens from the Universal Declaration of Human Rights.

% \item \textbf{L\d{\'e}s{\'i}k{\`a}} is a fifty thousand token corpus of unigrams taken from recent research by Victor Williamson on the Yor{\`u}b{\'a} dictionary. To this list we added de-duped unigrams from all the entire dataset, before splitting into training and validation. This gave us a comprehensive view and the ability to learn the diacritic patterns of all Yor{\`u}b{\'a} words as well as common loan-words.

% \item \textbf{Private texts}: In addition to the public domain texts, the authors also had access to personal archives of written work, correctly tone-marked that would add some meaningful diversity to the corpus. These included transcription of interviews and long form stories. These texts numbered some three hundred thousand tokens.

% TODO We should delete this part since, I don't really know what Timi did. I asked him for something we can use in December, but he hasn't given it, so I'm not sure it makes sense to include this.
% \item \textbf{{Yor{\`u}b{\'a} Object Character Recognition (OCR)}}: Another direction explored by the authors, was the use of Object Character Recognition (OCR).  This is the process of scanning physical books to derive plain text from the scanned images. However the presence of diacritics in Yor{\`u}b{\'a} books presents problems for English OCR models. So we retrained a new Yor{\`u}b{\'a} OCR model using existing clean text. TIMI elborate here on what was done. Feel free to blow grammer and give technical details.\. We bootstrapped our Háà Ènìyàn efforts, Ogboju Ode as well as Aaro Meta.
% \end{itemize}

 \begin{table}[h]
  \caption{Training data subsets}
  \label{tab:training_datasets}
  \centering
  \begin{tabular}{rll}
    \toprule
    \textbf{\# words} & \textbf{Source or URL}  & \textbf{Description} \\
    \midrule
    24,868 & rma.nwu.ac.za  & Lagos-NWU corpus \\  
    50,202 & theyorubablog.com & language blog\\  
    910,401 & bible.com/versions/911 & Biblica \\
    \midrule
    11,488,825 & opus.nlpl.eu & JW300 \\
    831,820 & bible.com/versions/207 & Bible Society Nigeria \\
    142,991 & - & Language ID corpus \\
    47,195 & - & Yor{\`u}b{\'a} Lexicon \\
    29,338 & yoruba.unl.edu & Proverbs \\
    28,308 & yo.globalvoices.org & Global Voices news \\
    2,887 & unicode.org/udhr & Human rights edict \\
    --- & YorubaTWI & YorubaTwi Embeddings \\
    \midrule
    150,360 & Private sources & Interview text \\
    15,243 & OCR & Short Stories \\
    910,401 & OCR & H{\'a}{\`a} {\`E}n{\`i}y{\`a}n \\

    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Experimental setup}\label{sec:experimental}
Data preprocessing and parallel text preparation followed the same procedures used in \citep{orife2018adr}. Experiments included evaluations of the effect of the various texts on the test accuracy, notably for JW300, which is a disproportionately large contributor to the dataset. We also evaluated models trained with pre-trained FastText embeddings to understand the boost in performance possible with word embeddings.

\paragraph{A new, modern multi-purpose evaluation dataset}\label{sec:evaldataset}
% TODO we need to write this properly
To succeed at the ADR task for users, our research experiments needed to be guided by a test set based around modern, non-archaic text in popular usage. After much review, there was a consensus that the journalistic news text best represented the modern, colloquial usage of the language. So we selected Global Voices, a multilingual community of journalists, translators, bloggers, academics and human rights activists. Their newsroom articles are translated into dozens of languages. We used a web-scrape of their Yor{\`u}b{\'a} articles comprising some twenty-eight thousand tokens.

% In the previous paper, the test set was selected as a subset of the total available dataset, this means that the test and validation sets, drew generally speaking from the same distribution as the training set, this explains the stellar performance reported in Orife, 2018. However, we need our evaluation set to be a (1) public (2) representative of a mixture of modern written and spoken Yoruba styles (3) large enough. 

% From the texts that we had available, including interviews, journalistic news, literary and liturgical texts,  Therefore, by evaluating the quality of the various models on this diacritic restoration task would ensure that published models would best match the language usage expectations of users. For these reasons we converged on using broadcast news (Iroyin) as our evaluation set.

\section{Results}\label{sec:results}
% TODO IRORO need to write this properly

The dev set will be the test set drawn from the training data's distribution. Test set will be Iroyin as described above. We should match the figures published in \citep{orife2018adr} with the current figure to tell a consistent story. Even though the test set has changed, we should still evaluate the legacy models on it to show how badly it geneneralized even thought it had very good in-domain performance.

% TODO IRORO DAVID these results can go in the Appendix --> we need to write this properly
To evaluate the performance of our ADR models, we computed the accuracy score as the ratio of correct words restored to all words. We calculate the perplexity of each model's predictions based on the test set targets.
Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. 

\section{Conclusions and Future Work}

Additional data and more diverse data definitely improves performance. Modern Text embeddings provide an additional boost in accuracy (TBD).

%Moving towards outputs
%Building killer apps
%Sharing the experience and expanding the NLP tasks

All public-domain datasets referenced in this work are available on GitHub.\footnote{\url{https://github.com/Niger-Volta-LTI/yoruba-text}} \footnote{\url{https://github.com/Niger-Volta-LTI/yoruba-adr}}

\subsubsection*{Acknowledgments}
% TODO IRORO DAVID Who else are we thanking?  
We thank everyone from Yor{\`u}b{\'a} Name, Masakhane, DataScience Nigeria. 

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\clearpage

\appendix
\section{Appendix}

 \begin{table}[h]
  \caption{Training \& Test Accuracy and Perplexity}
  \label{tab:results-appendix}
  \centering
  \begin{tabular}{lcc}
    \toprule
    \textbf{Model} & \textbf{Accuracy \%} &\textbf{Perplexity} \\
    \midrule
    Baseline RNN (?) & 90.1 & 1.68 \\
    Baseline Bandahau & 90.1 & 1.85 \\
    \midrule
	Bandahau+ & 90.1 & 1.9 \\ 
	Transformer+ & 90.1 & 1.9 \\ 
    \midrule
	Bandahau+JW300 & 90.1 & 1.9 \\ 
	Transformer+JW300 & 90.1 & 1.9 \\ 
	\midrule
	Bandahau+JW300+FastText & 90.1 & 1.9 \\ 
	Transformer+JW300+FastText & 90.1 & 1.9 \\ 
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
\caption{Example Sentences}
\label{results}
\begin{center}
  \begin{tabular}{ll}
  	\toprule
	 Source: &    Still , words of apology are a strong force toward making peace \\
	 Reference:  &   We can even ask God to ‘ create in us a pure heart \\
	 Prediction:  &   We can even ask God to ‘ create in us a pure heart \\
	 \midrule
	 Source: &    Still , words of apology are a strong force toward making peace \\
	 Reference:  &   We can even ask God to ‘ create in us a pure heart \\
	 Prediction:  &   We can even ask God to ‘ create in us a pure heart \\
     \midrule
     Source: &    Still , words of apology are a strong force toward making peace \\
	 Reference:  &   We can even ask God to ‘ create in us a pure heart \\
	 Prediction:  &   We can even ask God to ‘ create in us a pure heart \\
    \bottomrule

  \end{tabular}
\end{center}
\end{table}

\end{document}
`