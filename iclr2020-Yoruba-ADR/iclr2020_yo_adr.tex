
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}


\title{Improving Yor{\`u}b{\'a} Diacritic Restoration}


% This will be everyone who has (a) provided or cleaned text (b) contributed code
\author{David Ad{\'e}lan{\'i} \\
Saarland University\\
Niger-Volta LTI \\
\And
Iroro Fred \d{\`O}n\d{\`o}m\d{\`e} Orife \\
Niger-Volta LTI\\
\And
K\d{\'{o}}l\'{a} T\'{u}b\d{\`{o}}s\'{u}n \\
Yor{\`u}b{\'a} Name \\
\And
T{\`i}m{\'i}l\d{\'{e}}h{\`i}n Fasubaa \\
Yor{\`u}b{\'a} Name \\
Niger-Volta LTI \\
\And
W{\'u}r{\`a}\d{o}l{\'a} Oy{\`e}w{\`u}s{\`i} \\
Yor{\`u}b{\'a} Name \\
Niger-Volta LTI \\
\And
\d{O}l{\'a}mil{\'e}kan Wahab \\
Yor{\`u}b{\'a} Name \\
Niger-Volta LTI \\
\And
Victor Williamson \\
Yor{\`u}b{\'a} Name \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Yor{\`u}b{\'a} is a widely spoken West African language with a writing system rich in orthographic and tonal diacritics. They provide morphological information, are crucial for lexical disambiguation, pronunciation and are vital for any computational Speech or Natural Language Processing  tasks. However diacritic marks, known in Yor{\`u}b{\'a} as \emph{am{\'i} oh{\`u}n}, are commonly excluded from electronic texts due to limited device and application support as well as general education on proper usage.

Building on our previous work \citep{orife2018adr}, we report on recent efforts at dataset cultivation, including web-scraping, text cleaning, diacritic error correction as well as Optical Character Recognition (OCR) from books. By aggregating and improving disparate texts from the web, as well as text curated from personal libraries, we were able to significantly grow our clean Yor{\`u}b{\'a} dataset from a majority Bibilical text corpora of some 900K tokens from three sources to over 11M tokens from over a dozen sources. 
 
 We evaluate improved versions of our diacritic restoration models on a new, general purpose, public domain Yor{\`u}b{\'a} dataset of modern journalistic news text. We selected this text to be a multi-purpose corpus reflecting contemporary usage. As before, we have released all pre-trained models, datasets and source-code as an open-source project to advance efforts on Yor{\`u}b{\'a} language technology.\\
\end{abstract}

\section{Introduction}\label{sec:introduction}

Yor{\`u}b{\'a} is a tonal language spoken by more than 40 Million people in the countries of Nigeria, Benin and Togo in West Africa. There are an additional million speakers in the African diaspora, making it the most broadly spoken African language outside Africa \citep{yoruba_language}. The phonology is comprised of eighteen consonants \emph{({b}, {d}, {f}, {g}, {gb}, {h}, {j}, {k}, {l}, {m}, {n}, {p}, {r}, {s}, \d{s}, {t}, {w}, y)}, seven oral vowel \emph{({a}, {e}, \d{e}, \d{i}, {o}, \d{o}, {u})} and five nasal vowel phonemes \emph{({an}, \d{e}{n}, {in}, \d{o}{n}, {un})} with three kinds of tones realized on all vowels and syllabic nasal consonants \emph{({\'m}, {\'n})} \citep{akinlabi2004sound}. Yor{\`u}b{\'a} orthography makes notable use of tonal diacritics to designate tonal patterns, and orthographic diacritics like underdots for various language sounds \citep{adegbola2012quantifying, wells2000orthographic}.

On modern computing and communication platforms, the majority of Yor{\`u}b{\'a} text is written in plain ASCII, without diacritics. This has negative implications for the quality of any Machine Translation (MT), Natural Language Processing (NLP) or Automatic Speech Recognition (ASR) task. To unlock the potential for a robust ecosystem of \emph{Yor{\`u}b{\'a}-first} language technologies, Yor{\`u}b{\'a} text must be correctly represented on current and future computing environments. The ultimate objective of automatic diacritic restoration (ADR) systems is to facilitate text entry and text correction that motivates and encourages the correct orthography and promotes quotidian usage of the language in electronic media.

\subsection{Ambiguity in non-diacritized text}
The main challenge in non-diacritized text is that it is very ambiguous \citep{orife2018adr, adegbola2012quantifying, asahiah2017restoring, de2007automatic}. ADR, which goes by other names such as Unicodification \citep{scannell2011statistical} or deASCIIfication \citep{arslan2016deasciification}, is a process which attempts to resolve the ambiguity present in undiacritized text. Adegbola et al. state that for ADR the ``prevailing error factor is the number of valid alternative arrangements of the diacritical marks that can be applied to the vowels and syllabic nasals within the words" \citep{adegbola2012quantifying}. Analogously to the analyses performed in \citep{orife2018adr}, we quantify the ambiguity in our current training set by the percentage of all words that have diacritics, 85\%; the percentage of unique non-diacritized word types that have two or more diacritized forms, 32\%, and the lexical diffusion or \emph{LexDif} metric, which conveys the average number of alternatives for each non-diacritized word, 1.47. IOHAVOC.

\begin{table}[t]
\caption{Diacritic characters with their non-diacritic forms}
\label{ambiguity-table}
\begin{center}
  \begin{tabular}{lcl}
    \multicolumn{2}{c}{\bf Characters} & \textbf{Examples}  \\
	 \hline \\
    {\`a} {\'a} \v{a} & \textbf{a} & gb{\`a} \emph{(spread)}, gba \emph{(accept)}, gb{\'a} \emph{(hit)}    \\  
    {\`e} {\'e} \d{e} \d{\`e} \d{\'e} & \textbf{e} & {\`e}s{\`e} \emph{(dye)}, \d{e}s\d{\`e} \emph{(foot)}, es{\'e} \emph{(cat)}\\
    {\`i} {\'i} & \textbf{i} & {\`i}l{\`u} \emph{(drum)}, ilu \emph{(opener)}, {\`i}l{\'u} \emph{(town)}\\  
    {\`o} {\'o} \d{o} \d{\`o} \d{\'o} \v{o} & \textbf{o} & ar\d{o} \emph{(an invalid)}, ar{\'o} \emph{(indigo)}, {\`a}r{\`o} \emph{(hearth)}, {\`a}r\d{o} \emph{(funnel)}, {\`a}r\d{\`o} \emph{(catfish)}\\  
    {\`u} {\'u} \v{u} & \textbf{u} & k{\`u}n \emph{(to paint)}, kun \emph{(to carve)}, k{\'u}n \emph{(be full)} \\
	\hline \\
    {\`n} {\'n} \={n} & \textbf{n} & {\`n} (a negator), {n} \emph{(I)}, {\'n} (continuous aspect marker) \\  
    \d{s} & \textbf{s} &  \d{s}{\`a} \emph{(to choose)}, \d{s}{\'a} \emph{(fade)}, {s}{\`a} \emph{(to baptise)}, {s}{\'a} \emph{(to run)}\\  
	\hline \\
  \end{tabular}
\end{center}
\end{table}

Finally, 64\% of all unique, non-diacritized monosyllabic words possess multiple diacritized forms \citep{oluseye2003yoruba, delano1969dictionary}. Given that Yor{\`u}b{\'a} verbs are predominantly monosyllabic and that there are tonal changes rules governing how tonal diacritics on a specific word are \emph{altered} based on context, we acknowledge the complexity of the disambiguation task of diacritic restoration \citep{orife2018adr, delano1969dictionary}. 

\subsection{Improving generalization performance}

In our efforts to make the the first ADR models \citep{orife2018adr}, trained on majority Biblical text, available to a wider audience, we frequently tested on colloquial, conversational text. We observed that these early ADR models suffered from domain-mismatch generalization errors and appeared particularly not-robust when presented with contractions or variants of common phrases. We attributed these errors to low-diversity of sources, in this case just three, as well as not-enough-data, under a million tokens. To remedy this problem,  we set about to  we demonstrate that we can improve the model trained on majority Biblical text, using a lot more text from the a variety of sources.

This paper is organized as follows. Section 2 summarizes our data collection efforts. In section 3, we outline our experimental setup and models trained. In section 4, we present the results of the evaluation. In section 5, we conclude with an error analysis and future directions.

\section{Methodology}\label{sec:methods}

\subsection{Data Collection}\label{sec:collection}

We started by first attempting a comprehensive GitHub, literature and Google web search, assembling all the public-domain Yor{\`u}b{\'a} sources online. Those without admissible or consistent quality were put into a special queue for human supervision and corrections. This notably included Wikipedia and Twitter. Below we describe briefly each of the admitted corpora to convey a sense of the subject matter and word distributions. 


\begin{itemize}

\item \textbf{JW300}: JW300 is a large-scale a parallel corpus for Machine Translation (MT) comprising more than three hundred languages with on average one hundred thousand parallel sentences per language pair. The Yor{\`u}b{\'a}-English token pairs number some thirteen million. JW300 text is drawn from a number of online blogs, news and contemporary religious magazines by Jehovah's Witnesses (JW).

\item \textbf{B{\'i}b{\'e}l{\`i}}: Two different versions of the Yor{\`u}b{\'a} Bible were web-scraped cleaned. The first is from Biblica, which is a translation of the New International Version (NIV). The second version is a 2010 translation published by the Bible Society of Nigeria (BSN). Our previous work only used the Biblica version. A computational analysis of the word distributions of each Bible's text revealed that on average 60\% of all verses were not identical. Therefore, it valuable to have both versions of the Bible to give a diversity of expression for the same concepts. Further more, the additional size was is still dwarfed by the JW300 corpus. 

\item \textbf{Language identification corpus}: This medium sized corpus of one hundred and fifty thousand tokens was used in a Nigerian language identification task involving the three major Nigerian tongues, Hausa, Igbo and Yor{\`u}b{\'a}. We obtained the public-domain text from GitHub \citep{Asubiaro_langid}.

\item \textbf{Yor{\`u}b{\'a}-Twi word-embedding corpora}
This medium sized corpus comprises a number of texts used in a recent word embedding task 

\item \textbf{{\`O}we} is a small collection of Yor{\`u}b{\'a} proverbs scraped from the electronic version of \emph{The Good Person: Excerpts from the Yor{\`u}b{\'a} Proverb Treasury}, a collection compiled and translated by Dr. Oyekan Owomoyela at the University of Nebraska - Lincoln. The corpus has five thousand tokens and parallel English translations \citep{oweyoruba}.

\item \textbf{Universal Declaration of Human Rights} is a tiny corpus of some two thousand eight hundred tokens from the Universal Declaration of Human Rights.

\item \textbf{L\d{\'e}s{\'i}k{\`a}} is a fifty thousand token corpus of unigrams taken from recent research by Victor Williamson on the Yor{\`u}b{\'a} dictionary. To this list we added de-duped unigrams from all the entire dataset, before splitting into training and validation. This gave us a comprehensive view and the ability to learn the diacritic patterns of all Yor{\`u}b{\'a} words as well as common loan-words.

\item \textbf{Private texts}: In addition to the public domain texts, the authors also had access to personal archives of written work, correctly tone-marked that would add some meaningful diversity to the corpus. These included transcription of interviews and long form stories. These texts numbered some three hundred thousand tokens.

\item \textbf{{Yor{\`u}b{\'a} Object Character Recognition (OCR)}}: Another direction explored by the authors, was the use of Object Character Recognition (OCR).  This is the process of scanning physical books to derive plain text from the scanned images. However the presence of diacritics in Yor{\`u}b{\'a} books presents problems for English OCR models. So we retrained a new Yor{\`u}b{\'a} OCR model using existing clean text. TIMI elborate here on what was done. Feel free to blow grammer and give technical details.\. We bootstrapped our Háà Ènìyàn efforts, Ogboju Ode as well as Aaro Meta.
\end{itemize}

 \begin{table}[h]
  \caption{Training data subsets}
  \label{tab:training_datasets}
  \centering
  \begin{tabular}{rll}
    \toprule
    \textbf{\# words} & \textbf{Source or URL}  & \textbf{Description} \\
    \midrule
    24,868 & rma.nwu.ac.za  & Lagos-NWU corpus \\  
    50,202 & theyorubablog.com & language blog\\  
    910,401 & bible.com/versions/911 & Biblica \\
    \midrule
    11,488,825 & opus.nlpl.eu & JW300 \\
    831,820 & bible.com/versions/207 & Bible Society Nigeria \\
    142,991 & - & Language ID corpus \\
    47,195 & - & Yor{\`u}b{\'a} Lexicon \\
    29,338 & yoruba.unl.edu & Proverbs \\
    28,308 & yo.globalvoices.org & Global Voices news \\
    2,887 & unicode.org/udhr & Human rights edict \\

    \midrule
    150,360 & - & Interview text \\
    15,243 & - & Short Stories \\
    910,401 & - & Háà Ènìyàn \\

    \bottomrule
  \end{tabular}
\end{table}

\subsection{Experimental setup}\label{sec:experimental}

 When collecting data from disparate sources, we preprocessed texts to ensure consistent, error-free diacritization, splitting lines on full-stops to give one sentence per line. To ensure our splits are drawn from similar distributions, we combined all text, shuffled and split utterances into a ratio of 90\%, 10\%, for training and dev sets respectively.   

To prepare source and target texts for parallel training via a sequence-to-sequence architecture, all characters in the texts were dispossessed of their diacritics. Diacritized text was converted to Unicode Normalization Form Canonical Decomposition (NFD) which separates a base character from its diacritics. Next, \emph{UnicodeCategory.NonSpacingMark} characters, which house the diacritic modifications to a character, were filtered out. This yielded two sets of text, one stripped of diacritics (the source) and the other with full diacritics (the training target). 

To better understand the dataset split, we computed a perplexity of 575.6 for the test targets with a language model trained over the training targets \citep{stolcke2002srilm}. The \{source, target\} vocabularies for training and test sets have \{11857, 18979\} and \{4042, 5641\} word types respectively.

\subsection{Using pre-trained text embeddings}
TEXT TO FIX and TO DESCRIBE DAVID's FastText, BERT and XLM embeddings. FastText, BERT and XLM embeddingsFastText, BERT and XLM embeddingsFastText, BERT and XLM embeddingsFastText, BERT and XLM embeddingsFastText, BERT and XLM embeddingsFastText, BERT and XLM embeddingsFastText, BERT and XLM embeddings.


\subsection{A new, modern multi-purpose evaluation dataset}\label{sec:evaldataset}

Here we discuss in detail the selection of Iroyin, perhaps present some facts and figures about it, to better understand why it is a good evaluation set and to motivate how we converged on it being the best text subset. Modern, non-archaic text, popular usage and a better proxy than any of the religious centered texts (JW300, Bibeli, etc) and less formal.

Global Voices is a multilingual community of journalists, translators, bloggers, academics and human rights activists. Their newsroom articles are translated into dozens of languages. We used a web-scrape of their Yor{\`u}b{\'a} articles comprising some twenty-eight thousand tokens.

In the previous paper, the test set was selected as a subset of the total available dataset, this means that the test and validation sets, drew generally speaking from the same distribution as the training set, this explains the stellar performance reported in Orife, 2018. However, we need our evaluation set to be a (1) public (2) representative of a mixture of modern written and spoken Yoruba styles (3) large enough. 

From the texts that we had available, including interviews, journalistic news, literary and liturgical texts, there was a consensus that the journalistic news text best represented the modern, colloquial usage of the language. Therefore, by evaluating the quality of the various models on this diacritic restoration task would ensure that published models would best match the language usage expectations of users. For these reasons we converged on using broadcast news (Iroyin) as our evaluation set.


\subsection{Training}\label{sec:training}
We built all models with the Python 3 implementation of \texttt{OpenNMT-py}, an open-source toolkit created by the Klein et al. \citep{opennmt}. Our training hardware configuration was a standard AWS EC2 p2.xlarge instance with a NVIDIA K80 GPU, 4 vCPUs and 61GB RAM. Training the various models took place over the course of a few days.


\section{Results}\label{sec:results}


The dev set will be the test set drawn from the training data's distribution. Test set will be Iroyin as described above. We should match the figures published in \citep{orife2018adr} with the current figure to tell a consistent story. Even though the test set has changed, we should still evaluate the legacy models on it to show how badly it geneneralized even thought it had very good in-domain performance.


To evaluate the performance of our ADR models, we computed the accuracy score as the ratio of correct words restored to all words. We calculate the perplexity of each model's predictions based on the test set targets.
 \begin{table}[h]
  \caption{Training \& Test Accuracy and Perplexity}
  \label{tab:results}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Model} & \textbf{Train\%} & \textbf{Dev\%} & \textbf{Test\%} &\textbf{PPL} \\
    \midrule
    Baseline RNN & 96.2 & 90.1 & 90.1 & 1.68 \\
    Bandahau from [1] & 95.9 & 90.1 & 90.1 & 1.85 \\
    \midrule
	Bandahau++ & - & - & - & - \\ 
	Transformer++ & - & - & - & - \\ 
	\midrule
	Transformer++ FastText & - & - & - & - \\ 
	Transformer++ BERT & - & - & - & - \\ 
	Transformer++ XLM & - & - & - & - \\ 

    \bottomrule
  \end{tabular}
\end{table}
Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. Discussion of results. 

\subsection{Error Analysis}
While performing error analyses on the model predictions, we observed: DESCRIBE YOUR OBSERVATIONS

\section{Conclusions}


\subsection{Discussion} %a.k.a lessons learned
Additional data and more diverse data definitely improves performance. Modern Text embeddings provide an additional boost in accuracy (TBD).


\subsection{Future Work}
%Moving towards outputs
%Building killer apps
%Sharing the experience and expanding the NLP tasks

All public-domain datasets referenced in this work are available on GitHub.\footnote{\url{https://github.com/Niger-Volta-LTI/yoruba-text}} \footnote{\url{https://github.com/Niger-Volta-LTI/yoruba-adr}}

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\end{document}
